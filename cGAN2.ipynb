{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205, 6002)\n",
      "                                       ID      2000      2003      2006  \\\n",
      "200  47ef51be-3deb-4453-941c-08f3c84ccfcc  0.000472  0.000524  0.000650   \n",
      "201  406818f0-c75a-49d2-bca7-4cdb74a17926  0.000479  0.000470  0.000194   \n",
      "202  494d63a5-7879-4c7b-88e9-87cbece63710  0.000357  0.000161  0.000237   \n",
      "203  e5fc5463-df08-4500-b3f5-d723832a5491  0.000532  0.000514  0.000731   \n",
      "204  9d18bf07-46e5-4f4e-97fa-546f6b9dd589  0.000808  0.000809  0.000361   \n",
      "\n",
      "         2009      2012      2015      2018      2021      2024  ...  \\\n",
      "200  0.000457  0.000448  0.000564  0.000569  0.000381  0.000159  ...   \n",
      "201  0.000317  0.000426  0.000388  0.000566  0.000531  0.000343  ...   \n",
      "202  0.000258  0.000205  0.000203  0.000237  0.000105  0.000041  ...   \n",
      "203  0.000382  0.000134  0.000017  0.000082  0.000274  0.000338  ...   \n",
      "204  0.000259  0.000258  0.000397  0.000328  0.000201  0.000082  ...   \n",
      "\n",
      "        19973     19976     19979     19982         19985     19988     19991  \\\n",
      "200  0.000021  0.000033  0.000021  0.000025  3.161886e-05  0.000047  0.000033   \n",
      "201  0.000011  0.000020  0.000029  0.000031  4.483804e-05  0.000051  0.000101   \n",
      "202  0.000069  0.000107  0.000077  0.000081  8.134780e-05  0.000103  0.000046   \n",
      "203  0.000021  0.000018  0.000018  0.000048  4.528473e-05  0.000052  0.000048   \n",
      "204  0.000012  0.000016  0.000012  0.000006  7.485071e-07  0.000007  0.000012   \n",
      "\n",
      "        19994     19997  Meropenem  \n",
      "200  0.000032  0.000028          S  \n",
      "201  0.000082  0.000070          S  \n",
      "202  0.000033  0.000014          S  \n",
      "203  0.000058  0.000029          S  \n",
      "204  0.000014  0.000016          S  \n",
      "\n",
      "[5 rows x 6002 columns]\n",
      "Index(['ID', '2000', '2003', '2006', '2009', '2012', '2015', '2018', '2021',\n",
      "       '2024',\n",
      "       ...\n",
      "       '19973', '19976', '19979', '19982', '19985', '19988', '19991', '19994',\n",
      "       '19997', 'Meropenem'],\n",
      "      dtype='object', length=6002)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from numpy.random import randn\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "data = pd.read_csv(\"C:/Users/HP/Desktop/dati sintetici/Escherichia_coli_Meropenem.csv\", sep = '\\t')\n",
    "print (data.shape)\n",
    "print (data.tail())\n",
    "print (data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_16556\\1372132325.py:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X = data.drop(['Meropenem','ID'], 1)\n"
     ]
    }
   ],
   "source": [
    "#We will use all columns except the last column as features to train the model. Meropenem column will be used as the label of the model.\n",
    "#features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "label = ['Meropenem']\n",
    "X = data.drop(['Meropenem','ID'], 1)\n",
    "y = data[label] \n",
    "\n",
    "\n",
    "data = (X.to_numpy()).reshape(X.shape[0], 6000).astype('float64')\n",
    "#y = (y.to_numpy()).reshape(X.shape[0], 1).astype('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    X = generator.predict(x_input)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# generate n real samples with class labels; We randomly select n samples from the real data\n",
    "def generate_real_samples(n):\n",
    "    X1 = X.sample(n)\n",
    "    #X = np.random.Generator.choice(data, size = n)   \n",
    "    y = np.ones((n, 1))\n",
    "    return X1, y\n",
    "\n",
    "def define_generator(latent_dim, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, activation='relu',  kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='linear'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def define_discriminator(n_inputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #model.summary()\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=100, n_batch=20, n_eval=200):\n",
    "    # determine half the size of one batch, for updating the  discriminator\n",
    "    half_batch = int(n_batch / 2)\n",
    "    d_history = []\n",
    "    g_history = []\n",
    "    # manually enumerate epochs\n",
    "    for epoch in range(n_epochs):\n",
    "    \n",
    "        # prepare real samples\n",
    "        x_real, y_real = generate_real_samples(half_batch)\n",
    "        # prepare fake examples\n",
    "        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        # update discriminator\n",
    "        d_loss_real, d_real_acc = d_model.train_on_batch(x_real, y_real)\n",
    "        d_loss_fake, d_fake_acc = d_model.train_on_batch(x_fake, y_fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        # prepare points in latent space as input for the generator\n",
    "        x_gan = generate_latent_points(latent_dim, n_batch)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = np.ones((n_batch, 1))\n",
    "        # update the generator via the discriminator's error\n",
    "        g_loss_fake = gan_model.train_on_batch(x_gan, y_gan)\n",
    "        print('>%d, d1=%.3f, d2=%.3f d=%.3f g=%.3f' % (epoch+1, d_loss_real, d_loss_fake, d_loss,  g_loss_fake))\n",
    "        d_history.append(d_loss)\n",
    "        g_history.append(g_loss_fake)\n",
    "        plot_history(d_history, g_history)\n",
    "        g_model.save('trained_generated_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_76 (Dense)            (None, 15)                165       \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 30)                480       \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 10)                310       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 955\n",
      "Trainable params: 955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 181ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_37\" is incompatible with the layer: expected shape=(None, 10), found shape=(10, 6000)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m gan_model \u001b[39m=\u001b[39m define_gan(generator, discriminator)\n\u001b[0;32m      9\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m train(generator, discriminator, gan_model, latent_dim)\n",
      "Cell \u001b[1;32mIn[48], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(g_model, d_model, gan_model, latent_dim, n_epochs, n_batch, n_eval)\u001b[0m\n\u001b[0;32m     13\u001b[0m x_fake, y_fake \u001b[39m=\u001b[39m generate_fake_samples(g_model, latent_dim, half_batch)\n\u001b[0;32m     14\u001b[0m \u001b[39m# update discriminator\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m d_loss_real, d_real_acc \u001b[39m=\u001b[39m d_model\u001b[39m.\u001b[39;49mtrain_on_batch(x_real, y_real)\n\u001b[0;32m     16\u001b[0m d_loss_fake, d_fake_acc \u001b[39m=\u001b[39m d_model\u001b[39m.\u001b[39mtrain_on_batch(x_fake, y_fake)\n\u001b[0;32m     17\u001b[0m d_loss \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39madd(d_loss_real, d_loss_fake)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:2510\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2506\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2507\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2508\u001b[0m     )\n\u001b[0;32m   2509\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2510\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   2512\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2513\u001b[0m \u001b[39mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filefn7in6b5.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:1268\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1264\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[0;32m   1265\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1266\u001b[0m     )\n\u001b[0;32m   1267\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1268\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[0;32m   1269\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1270\u001b[0m     outputs,\n\u001b[0;32m   1271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[0;32m   1272\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[0;32m   1273\u001b[0m )\n\u001b[0;32m   1274\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:1249\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1249\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[0;32m   1250\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:1050\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# Run forward pass.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m-> 1050\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1051\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[0;32m   1052\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\input_spec.py:298\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m spec_dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m     \u001b[39mif\u001b[39;00m spec_dim \u001b[39m!=\u001b[39m dim:\n\u001b[1;32m--> 298\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    299\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    300\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mincompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected shape=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound shape=\u001b[39m\u001b[39m{\u001b[39;00mdisplay_shape(x\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_37\" is incompatible with the layer: expected shape=(None, 10), found shape=(10, 6000)\n"
     ]
    }
   ],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 10\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator(n_inputs=10)\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim, n_outputs= 10)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# train model\n",
    "train(generator, discriminator, gan_model, latent_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
